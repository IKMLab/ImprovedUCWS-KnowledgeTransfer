r"""The second stage of the framework.
To enhance word segmentation performance by leveraging pre-trained knowledge,
a BERT-based classifier is trained using the pseudo-labels generated by the
segment model. The classifier accomplishes the word segmentation task by
assigning a position within a word to each character.
"""

import argparse
import logging
import os
from time import time

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from codes import (CWSHugTokenizer, InputDataset, OneShotIterator,
    SegmentClassifier, set_seed, set_logger, eval, get_optimizer_and_scheduler)


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch_size", type=int, default=12000)
    parser.add_argument("--train_steps", type=int, default=1500)
    parser.add_argument("--data_name", type=str, default='as')
    parser.add_argument("--file_name", type=str, default='prediction.txt')
    parser.add_argument("--early_stop_threshold", type=int, default=4)
    parser.add_argument("--exp", type=str, default='models_42_pipeline_lstm_cls_refactor')
    parser.add_argument("--hug_name", type=str, default='bert-base-chinese')
    parser.add_argument("--num_labels", type=int, default=2, help="Choose tagging schema: 2 for word boundary and 4 for BMES.")
    parser.add_argument("--save_dir", type=str, default='cls_output')
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--save_every_steps", type=int, default=4)
    parser.add_argument("--lr_rate", type=float, default=5e-5)
    parser.add_argument("--max_norm", type=float, default=0.1)

    parser.add_argument("--is_cls_blank", action='store_true', help="Whether classifier bert model is empty.")
    parser.add_argument("--is_bi", action='store_true', help="Whether use B/I (Begin/Inside) tagging schema.")
    parser.add_argument("--is_c_nc", action='store_true', help="Whether use SpIn-WS tagging schema (Connect or not).")
    parser.add_argument("--is_mlm", action='store_true', help="Whether use masked language model predction.")
    parser.add_argument("--mlm_weight", type=float, default=1.0)
    parser.add_argument("--init_cls_model", type=str, default=None)
    parser.add_argument("--init_cls_encoder", type=str, default=None)

    return parser.parse_args()


def main():

    args = get_args()

    # Save path.
    out_dir = f'{args.save_dir}/cls_{args.data_name}'
    os.makedirs(out_dir, exist_ok=True)

    set_logger(f'{out_dir}/train.log')

    set_seed(args.seed)

    logging.info(f'args: {args}')

    DATA_PATH = f'data/{args.data_name}'
    SCRIPT = f'data/score.pl'
    TRAINING_WORDS = f'{DATA_PATH}/words.txt'
    GOLD_TEST = f'{DATA_PATH}/test_gold.txt'

    # cls output and score.
    CLS_VALID_OUTPUT = f'{out_dir}/valid_prediction_cls.txt'
    CLS_VALID_SCORE = f'{out_dir}/valid_score_cls.txt'

    eval_command = f'perl {SCRIPT} {TRAINING_WORDS} {GOLD_TEST} {CLS_VALID_OUTPUT}'

    device = torch.device('cuda:0')

    vocab_file = 'data/vocab/vocab.txt'
    tk = CWSHugTokenizer(
        vocab_file=vocab_file,
        tk_hug_name=args.hug_name
    )
    cls_model = SegmentClassifier(
        embedding_size=None,
        vocab_size=len(tk),
        init_embedding=None,
        d_model=768,
        d_ff=None,
        dropout=0.1,
        n_layers=None,
        n_heads=None,
        model_type=args.hug_name,
        pad_id=tk.pad_id,
        tk_cls=tk,
        encoder=None,
        num_labels=args.num_labels,
        label_smoothing=0.0,
        is_blank=args.is_cls_blank,
        is_c_nc=args.is_c_nc,
        is_mlm=args.is_mlm,
    )

    cls_model.to(device)
    cls_model.train()

    cls_optimizer, cls_scheduler = get_optimizer_and_scheduler(
        model=cls_model,
        lr_rate=args.lr_rate,
        optimizer_type='Adam',
        lr_lambda=lambda step: 1 if step < 0.8 * args.train_steps else 0.1,
    )

    logging.info(f'==== Model Info ====')
    logging.info(cls_model)
    logging.info(f'Tokenizer vocab_size: {len(tk)}')

    # Training dataset, dataloader and iterator.
    train_inputs = [f'{args.exp}/{args.file_name}']
    train_dset = InputDataset(
        train_inputs,
        tk,
        is_training=True,
        batch_token_size=args.batch_size
    )
    train_dldr = DataLoader(
        train_dset,
        num_workers=4,
        batch_size=1,
        shuffle=False,
        collate_fn=InputDataset.single_collate
    )
    train_iter = OneShotIterator(train_dldr)

    # Valid dataloader.
    valid_inputs = [f'data/{args.data_name}/test.txt']
    valid_dset = InputDataset(valid_inputs, tk)
    valid_dldr = DataLoader(
        dataset=valid_dset,
        shuffle=False,
        batch_size=args.batch_size,
        num_workers=4,
        collate_fn=InputDataset.padding_collate
    )

    logging.info('==== Dataset ====')
    logging.info(f'train_inputs: {train_inputs}')
    logging.info(f'valid_inputs: {valid_inputs}')

    is_bmes = True if args.num_labels == 4 else False
    is_bi = args.is_bi
    if is_bmes == True and is_bi == True:
        raise ValueError("You can only choose one tagging schema. (BI or BMES)")

    # Log on the tensorboard.
    writer = SummaryWriter(out_dir)

    best_F_score_cls = 0
    early_stop_counts, early_stop_threshold = 0, args.early_stop_threshold

    tqdm_bar = tqdm(range(args.train_steps), dynamic_ncols=True)
    for step in tqdm_bar:
        cls_model.train()

        x_batch, seq_len_batch, uchars_batch, segments_batch = next(train_iter)

        # Generate the classifier's labels.
        # `batch_labels_cls` shape: (B, S)
        batch_labels_cls = cls_model.generate_label(
            x_batch, segments_batch, tk=tk, is_bmes=is_bmes, is_bi=is_bi)

        x_batch = x_batch.to(device)
        loss = cls_model(x=x_batch, labels=batch_labels_cls.to(device))

        # Masked language modeling loss or not.
        if args.is_mlm:
            mlm_loss = cls_model.mlm_forward(x_batch)
            loss = loss + mlm_loss * args.mlm_weight
            tqdm_bar.set_description(f'step: {step}, loss: {loss.item():.4f}, mlm_loss: {mlm_loss.item():.4f}')
        else:
            loss = loss

        loss.backward()

        nn.utils.clip_grad_norm_(cls_model.parameters(), args.max_norm)
        cls_optimizer.step()
        cls_scheduler.step()
        cls_model.zero_grad()
        cls_optimizer.zero_grad()

        tqdm_bar.set_description(f'step: {step}, loss: {loss.item():.4f}')

        writer.add_scalar('loss', loss.item(), step)
        writer.add_scalar('mlm_loss', loss.item(), step)
        writer.add_scalar('lr', cls_scheduler.get_last_lr()[0], step)

        if (step+1) % args.save_every_steps == 0: # and step >= 12:
            cls_model.eval()

            fout_cls = open(CLS_VALID_OUTPUT, 'w')

            with torch.no_grad():
                # Do validation.
                for x_batch, seq_len_batch, uchars_batch, segments_batch, restore_orders in valid_dldr:
                    x_batch = x_batch.to(device)

                    # Generate segmentation results from classifier.
                    segments_batch_cls = cls_model.generate_segments(
                        x=x_batch, lengths=seq_len_batch, is_bi=is_bi)
                    for i in restore_orders:
                        uchars, segments_cls = uchars_batch[i], segments_batch_cls[i]
                        fout_cls.write(tk.restore(uchars, segments_cls))

            fout_cls.close()

            F_score_cls = eval(eval_command, CLS_VALID_SCORE, True)
            writer.add_scalar('F_score_cls', F_score_cls, step)
            # logging.info(f'=== step: {step}, F_score_cls: {F_score_cls:.4f} ===')

            if F_score_cls > best_F_score_cls:
                best_F_score_cls = F_score_cls
                logging.info('Saving checkpoint %s...' % out_dir)
                torch.save({
                    'global_step': step,
                    'best_F_score': best_F_score_cls,
                    'model_state_dict': cls_model.state_dict(),
                    'adam_optimizer': cls_optimizer.state_dict()
                }, os.path.join(out_dir, 'best-cls_checkpoint'))
                early_stop_counts = 0

                logging.info(f'step: {step}, loss: {loss.item()}, best_F_score: {best_F_score_cls:.4f}')

            elif F_score_cls < best_F_score_cls:
                early_stop_counts += 1

            if early_stop_counts >= early_stop_threshold:
                logging.info(f'Early stop at step {step}.')
                break

            cls_model.train()

    writer.close()

    # Load the best cls_model checkpoint for word segmentation on the test set.
    TEST_OUTPUT = f'{out_dir}/prediction_cls.txt'
    TEST_SCORE = f'{out_dir}/score_cls.txt'
    eval_command_test = f'perl {SCRIPT} {TRAINING_WORDS} {GOLD_TEST} {TEST_OUTPUT}'

    # Load the best-cls_checkpoint.
    checkpoint = torch.load(f'{out_dir}/best-cls_checkpoint')
    cls_model.load_state_dict(checkpoint['model_state_dict'])
    cls_model.eval()

    fout_cls = open(TEST_OUTPUT, 'w')

    with torch.no_grad():
        # Do validation.
        for x_batch, seq_len_batch, uchars_batch, segments_batch, restore_orders in valid_dldr:
            x_batch = x_batch.to(device)

            # Generate segmentation results from classifier.
            segments_batch_cls = cls_model.generate_segments(
                x=x_batch, lengths=seq_len_batch, is_bi=is_bi)
            for i in restore_orders:
                uchars, segments_cls = uchars_batch[i], segments_batch_cls[i]
                fout_cls.write(tk.restore(uchars, segments_cls))

    fout_cls.close()

    F_score_cls = eval(eval_command_test, TEST_SCORE, True)


if __name__ == "__main__":

    start_time = time()

    main()

    print(f'Process time: {time() - start_time }')